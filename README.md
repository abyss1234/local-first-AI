# Local-first RAG + Agent Playground

A small end-to-end **local AI stack**:

- ğŸ§  Local LLM (Ollama)  
- ğŸ“š RAG with Qdrant  
- ğŸ•¹ Agent with simple tools (notes, TODOs)  
- âœ… Tests, basic eval & logs  

Everything runs **locally**, no external API keys.

---

## What is this?

A **minimal template** for building real AI apps on your own machine:

- Chat with a local model  
- Ask questions grounded on your **own docs**  
- Let an **Agent** call tools to do small tasks (write notes, make TODO list)  
- Have tests, simple eval, and logs so it feels closer to â€œproductionâ€ than a demo

---

## Why?

- **Local-first** â†’ privacy & full control  
- **Simple but complete** â†’ model + vector DB + API + UI in one repo  
- **Good habits** â†’ tests, eval, logging, tracing hooks all in place

---

## Features

- **FastAPI API**
  - `GET /health` â€“ health check  
  - `POST /chat` â€“ normal chat with local LLM  
  - `POST /chat_rag` â€“ RAG chat (answer + citations)  
  - `POST /agent` â€“ Agent that:
    - makes a simple plan  
    - calls tools from an allowlist  
    - returns `answer + citations + note_path + todos`

- **Agent tools (examples)**
  - `search_docs(query, top_k)` â€“ search Qdrant  
  - `write_note(title, content)` â€“ write markdown to `./notes`  
  - `make_todo_from_answer(text)` â€“ structured TODO list (JSON)

- **RAG & ingest**
  - `cli/ingest.py` â†’ parse `./docs`, chunk â†’ embed â†’ store in Qdrant

- **Eval (optional)**
  - `eval/questions.jsonl` â€“ small local test set  
  - `eval/run_eval.py` â€“ computes simple RAG metrics (e.g. context precision/recall)

- **Observability**
  - `logs/requests.jsonl` â€“ request latency, status, errors  
  - `logs/traces.jsonl` â€“ Agent audit log (tools, inputs, outputs)

- **UI (Streamlit)**
  - `Chat` â†’ `/chat`  
  - `RAG Chat` â†’ `/chat_rag` (with citations)  
  - `Agent` â†’ `/agent` (shows plan, tools, TODOs, note path)

---

## Stack

- **Model**: Ollama (`smollm2:135m` for chat, `all-minilm` for embeddings)  
- **Vector DB**: Qdrant (Docker)  
- **API**: FastAPI (`api/`)  
- **UI**: Streamlit (`ui/app.py`)  
- **Orchestration**: `docker-compose.yml`

---

## Setup

### 1. Start backend (Ollama + Qdrant + API)

From project root:

```bash
docker compose up -d --build

# Pull models:
docker compose exec ollama ollama pull smollm2:135m
docker compose exec ollama ollama pull all-minilm

# Ingest docs:
docker compose exec api python /app/cli/ingest.py --docs /app/docs

# Run test:
docker compose exec api pytest -q

# UI:
cd ui
pip install streamlit requests
streamlit run app.py
```
---

## Folder Structure
```
local-first-ai/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py          # FastAPI application (/chat, /chat_rag, /agentâ€¦)
â”‚   â”œâ”€â”€ rag.py           # RAG logic (embeddings + Qdrant)
â”‚   â”œâ”€â”€ agent_tools.py   # Agent tools, allowlist, and schema validation
â”‚   â””â”€â”€ observability/   # OpenTelemetry + request logging
â”‚
â”œâ”€â”€ cli/
â”‚   â””â”€â”€ ingest.py        # Ingest documents from ./docs into Qdrant
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ ...              # Example documents used by the system
â”‚
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ questions.jsonl  # Simple evaluation set (QA + expected sources)
â”‚   â””â”€â”€ run_eval.py      # Computes metrics and produces report.md
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ ...              # pytest tests (including prompt-injection tests)
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ app.py           # Streamlit UI
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ traces.jsonl     # Agent audit logs
â”‚   â””â”€â”€ requests.jsonl   # Request logs
â”‚
â”œâ”€â”€ notes/
â”‚   â””â”€â”€ ...              # Markdown notes generated by the Agent
â”‚
â””â”€â”€ docker-compose.yml
```
